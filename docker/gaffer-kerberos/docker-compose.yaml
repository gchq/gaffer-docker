# Copyright 2022 Crown Copyright
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

version: "3.7"

networks:
  default:
    name: gaffer

services:

  kdc-server:
    image: gchq/kdc:0.1-alpha
    healthcheck:
      test: bash -c 'until kadmin.local list_principals $HADOOP_PRINCIPLE*; do sleep 5; done; echo KDC is up;'
    build:
      context: ./kdc
    container_name: kdc-server
    hostname: kdc-server
    environment:
      - HADOOP_PRINCIPLE=${HADOOP_PRINCIPLE}
      - HADOOP_KRB_PASSWORD=${HADOOP_KRB_PASSWORD}
      - ZOOKEEPER_PRINCIPLE=${ZOOKEEPER_PRINCIPLE}
      - ZOOKEEPER_KRB_PASSWORD=${ZOOKEEPER_KRB_PASSWORD}
      - ACCUMULO_PRINCIPLE=${ACCUMULO_PRINCIPLE}
      - ACCUMULO_KRB_PASSWORD=${ACCUMULO_KRB_PASSWORD}
      - REALM=${REALM}
    volumes:
      - ./kdc/conf/krb5.conf:/etc/krb5.conf:ro
      - ./kdc/conf/kdc.conf:/etc/kdc.conf:ro
      - /var/log/krb5

  hdfs-namenode:
    image: gchq/hdfs-krb:${HADOOP_VERSION}
    build:
      context: ./hdfs-krb
      args:
        BASE_IMAGE_NAME: gchq/hdfs
        BASE_IMAGE_TAG: ${HADOOP_VERSION}
    command: namenode
    container_name: hdfs-namenode
    hostname: hdfs-namenode
    depends_on:
      kdc-server:
        condition: service_healthy
    environment:
      - HADOOP_CONF_DIR=${HADOOP_CONF_DIR}
      - HADOOP_PRINCIPLE=${HADOOP_PRINCIPLE}
      - HADOOP_KRB_PASSWORD=${HADOOP_KRB_PASSWORD}
      - REALM=${REALM}
      - DEBUG=${DEBUG}
    ports:
      - "9870:9870"
      - "9000:9000"
    volumes:
      - ./hdfs-krb/conf:${HADOOP_CONF_DIR}:ro
      - ./kdc/conf/krb5.conf:/etc/krb5.conf:ro
      - /var/log/hadoop
      - /data1
      - /data2

  hdfs-datanode:
    depends_on:
      - hdfs-namenode
    image: gchq/hdfs-krb:${HADOOP_VERSION}
    build:
      context: ./hdfs-krb
      args:
        BASE_IMAGE_NAME: gchq/hdfs
        BASE_IMAGE_TAG: ${HADOOP_VERSION}
    command: datanode
    container_name: hdfs-datanode
    hostname: hdfs-datanode
    environment:
      - HADOOP_CONF_DIR=${HADOOP_CONF_DIR}
      - HADOOP_PRINCIPLE=${HADOOP_PRINCIPLE}
      - HADOOP_KRB_PASSWORD=${HADOOP_KRB_PASSWORD}
      - REALM=${REALM}
      - DEBUG=${DEBUG}
    ports:
      - "1004:1004"
      - "1006:1006"
    volumes:
      - ./hdfs-krb/conf:${HADOOP_CONF_DIR}:ro
      - ./kdc/conf/krb5.conf:/etc/krb5.conf:ro
      - /var/log/hadoop
      - /data1
      - /data2

  zookeeper:
    depends_on:
      - kdc-server
    image: zookeeper:${ZOOKEEPER_VERSION}-krb
    build:
      context: ./zookeeper
      args:
        BASE_IMAGE_NAME: zookeeper
        BASE_IMAGE_TAG: ${ZOOKEEPER_VERSION}
    container_name: zookeeper
    hostname: zookeeper
    environment:
      - ZOO_SERVERS=server.1=zookeeper:2888:3888;2181
      - ZOO_4LW_COMMANDS_WHITELIST=*
      - ZOOKEEPER_PRINCIPLE=${ZOOKEEPER_PRINCIPLE}
      - ZOOKEEPER_KRB_PASSWORD=${ZOOKEEPER_KRB_PASSWORD}
    volumes:
      - ./zookeeper/conf/jaas.conf:/conf/jaas.conf:ro
      - ./zookeeper/conf/zoo.cfg:/conf/zoo.cfg:ro
      - ./zookeeper/conf/java.env:/conf/java.env:ro
      - ./kdc/conf/krb5.conf:/etc/krb5.conf:ro
      - /data
      - /datalog

#  accumulo-master:
#    depends_on:
#      - hdfs-namenode
#      - hdfs-datanode
#      - zookeeper
#    image: gchq/gaffer:${GAFFER_VERSION}
#    build:
#      context: .
#      args:
#        GAFFER_VERSION: ${GAFFER_VERSION}
#        BASE_IMAGE_NAME: gchq/accumulo
#        BASE_IMAGE_TAG: ${ACCUMULO_VERSION}
#    command: master
#    restart: on-failure
#    container_name: accumulo-master
#    hostname: accumulo-master
#    environment:
#      - ACCUMULO_CONF_DIR=${ACCUMULO_CONF_DIR}
#      # There doesn't seem to be an easy way (with docker-compose) to init our
#      # HDFS instance with the right permissions so that Accumulo can create the
#      # file structure it needs. Using the following workaround to allow
#      # accumulo to "auth" with HDFS as the super user so that it can:
#      - HADOOP_USER_NAME=hadoop
#    volumes:
#      - ../accumulo/conf:${ACCUMULO_CONF_DIR}:ro
#      - /var/log/accumulo
#
#  accumulo-tserver:
#    depends_on:
#      - accumulo-master
#    image: gchq/gaffer:${GAFFER_VERSION}
#    command: tserver
#    restart: on-failure
#    container_name: accumulo-tserver
#    hostname: accumulo-tserver
#    environment:
#      - ACCUMULO_CONF_DIR=${ACCUMULO_CONF_DIR}
#      # There doesn't seem to be an easy way (with docker-compose) to init our
#      # HDFS instance with the right permissions so that Accumulo can create the
#      # file structure it needs. Using the following workaround to allow
#      # accumulo to "auth" with HDFS as the super user so that it can:
#      - HADOOP_USER_NAME=hadoop
#    volumes:
#      - ../accumulo/conf:${ACCUMULO_CONF_DIR}:ro
#      - /var/log/accumulo
#
#  accumulo-monitor:
#    depends_on:
#      - accumulo-master
#    image: gchq/gaffer:${GAFFER_VERSION}
#    command: monitor
#    restart: on-failure
#    container_name: accumulo-monitor
#    hostname: accumulo-monitor
#    environment:
#      - ACCUMULO_CONF_DIR=${ACCUMULO_CONF_DIR}
#      # There doesn't seem to be an easy way (with docker-compose) to init our
#      # HDFS instance with the right permissions so that Accumulo can create the
#      # file structure it needs. Using the following workaround to allow
#      # accumulo to "auth" with HDFS as the super user so that it can:
#      - HADOOP_USER_NAME=hadoop
#    ports:
#      - 9995:9995
#    volumes:
#      - ../accumulo/conf:${ACCUMULO_CONF_DIR}:ro
#      - /var/log/accumulo
#
#  accumulo-gc:
#    depends_on:
#      - accumulo-master
#    image: gchq/gaffer:${GAFFER_VERSION}
#    command: gc
#    restart: on-failure
#    container_name: accumulo-gc
#    hostname: accumulo-gc
#    environment:
#      - ACCUMULO_CONF_DIR=${ACCUMULO_CONF_DIR}
#      # There doesn't seem to be an easy way (with docker-compose) to init our
#      # HDFS instance with the right permissions so that Accumulo can create the
#      # file structure it needs. Using the following workaround to allow
#      # accumulo to "auth" with HDFS as the super user so that it can:
#      - HADOOP_USER_NAME=hadoop
#    volumes:
#      - ../accumulo/conf:${ACCUMULO_CONF_DIR}:ro
#      - /var/log/accumulo
#
#  gaffer-rest:
#    image: gchq/gaffer-rest:${GAFFER_VERSION}
#    build:
#      context: ../gaffer-rest/
#      args:
#        GAFFER_VERSION: ${GAFFER_VERSION}
#    ports:
#      - 8080:8080
#    volumes:
#      - ./conf/store.properties:/gaffer/store/store.properties:ro
#
#  gaffer-ui:
#    image: gchq/gaffer-ui:${GAFFER_TOOLS_VERSION}
#    build:
#      context: ../gaffer-ui/
#      args:
#        GAFFER_TOOLS_VERSION: ${GAFFER_TOOLS_VERSION}
#    restart: always
#    ports:
#      - 5000:8080
#    volumes:
#      - ../gaffer-ui/config/ui/config.json:/opt/jboss/wildfly/standalone/deployments/ui.war/config/config.json:ro
