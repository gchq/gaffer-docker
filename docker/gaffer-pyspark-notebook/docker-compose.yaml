# Copyright 2020-2023 Crown Copyright
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

version: "3.7"

services:

  zookeeper:
    image: zookeeper:${ZOOKEEPER_VERSION}
    container_name: zookeeper
    hostname: zookeeper
    environment:
    - ZOO_SERVERS=server.1=zookeeper:2888:3888;2181
    - ZOO_4LW_COMMANDS_WHITELIST=*
    volumes:
    - /data
    - /datalog

  hdfs-namenode:
    image: gchq/hdfs:${HADOOP_VERSION}
    build:
      context: ../hdfs/
      args:
        HADOOP_VERSION: ${HADOOP_VERSION}
    command: namenode
    container_name: hdfs-namenode
    hostname: hdfs-namenode
    environment:
    - HADOOP_CONF_DIR=${HADOOP_CONF_DIR}
    ports:
    - 9870:9870
    volumes:
    - ../hdfs/conf:${HADOOP_CONF_DIR}:ro
    - /var/log/hadoop
    - /data1
    - /data2

  hdfs-datanode:
    depends_on:
    - hdfs-namenode
    image: gchq/hdfs:${HADOOP_VERSION}
    command: datanode
    container_name: hdfs-datanode
    hostname: hdfs-datanode
    environment:
    - HADOOP_CONF_DIR=${HADOOP_CONF_DIR}
    volumes:
    - ../hdfs/conf:${HADOOP_CONF_DIR}:ro
    - /var/log/hadoop
    - /data1
    - /data2

  accumulo-master:
    depends_on:
    - hdfs-namenode
    - hdfs-datanode
    - zookeeper
    image: gchq/gaffer:${GAFFER_VERSION}-accumulo-${ACCUMULO_VERSION}
    build:
      context: ../gaffer/
      args:
        GAFFER_VERSION: ${GAFFER_VERSION}
        BASE_IMAGE_NAME: gchq/accumulo
        BASE_IMAGE_TAG: ${ACCUMULO_VERSION}
    command: master
    restart: on-failure
    container_name: accumulo-master
    hostname: accumulo-master
    environment:
    - ACCUMULO_CONF_DIR=${ACCUMULO_CONF_DIR}
    # There doesn't seem to be an easy way (with docker-compose) to init our
    # HDFS instance with the right permissions so that Accumulo can create the
    # file structure it needs. Using the following workaround to allow
    # accumulo to "auth" with HDFS as the super user so that it can:
    - HADOOP_USER_NAME=hadoop
    volumes:
    - ../accumulo/conf-${ACCUMULO_VERSION}:${ACCUMULO_CONF_DIR}:ro
    - /var/log/accumulo

  accumulo-tserver:
    depends_on:
    - accumulo-master
    image: gchq/gaffer:${GAFFER_VERSION}-accumulo-${ACCUMULO_VERSION}
    command: tserver
    restart: on-failure
    container_name: accumulo-tserver
    hostname: accumulo-tserver
    environment:
    - ACCUMULO_CONF_DIR=${ACCUMULO_CONF_DIR}
    # There doesn't seem to be an easy way (with docker-compose) to init our
    # HDFS instance with the right permissions so that Accumulo can create the
    # file structure it needs. Using the following workaround to allow
    # accumulo to "auth" with HDFS as the super user so that it can:
    - HADOOP_USER_NAME=hadoop
    volumes:
    - ../accumulo/conf-${ACCUMULO_VERSION}:${ACCUMULO_CONF_DIR}:ro
    - /var/log/accumulo

  accumulo-monitor:
    depends_on:
    - accumulo-master
    image: gchq/gaffer:${GAFFER_VERSION}-accumulo-${ACCUMULO_VERSION}
    command: monitor
    restart: on-failure
    container_name: accumulo-monitor
    hostname: accumulo-monitor
    environment:
    - ACCUMULO_CONF_DIR=${ACCUMULO_CONF_DIR}
    # There doesn't seem to be an easy way (with docker-compose) to init our
    # HDFS instance with the right permissions so that Accumulo can create the
    # file structure it needs. Using the following workaround to allow
    # accumulo to "auth" with HDFS as the super user so that it can:
    - HADOOP_USER_NAME=hadoop
    ports:
    - 9995:9995
    volumes:
    - ../accumulo/conf-${ACCUMULO_VERSION}:${ACCUMULO_CONF_DIR}:ro
    - /var/log/accumulo

  accumulo-gc:
    depends_on:
    - accumulo-master
    image: gchq/gaffer:${GAFFER_VERSION}-accumulo-${ACCUMULO_VERSION}
    command: gc
    restart: on-failure
    container_name: accumulo-gc
    hostname: accumulo-gc
    environment:
    - ACCUMULO_CONF_DIR=${ACCUMULO_CONF_DIR}
    # There doesn't seem to be an easy way (with docker-compose) to init our
    # HDFS instance with the right permissions so that Accumulo can create the
    # file structure it needs. Using the following workaround to allow
    # accumulo to "auth" with HDFS as the super user so that it can:
    - HADOOP_USER_NAME=hadoop
    volumes:
    - ../accumulo/conf-${ACCUMULO_VERSION}:${ACCUMULO_CONF_DIR}:ro
    - /var/log/accumulo

  gaffer-rest:
    image: gchq/gaffer-rest:${GAFFER_VERSION}-accumulo-${ACCUMULO_VERSION}
    restart: on-failure
    container_name: gaffer-rest
    hostname: gaffer-rest
    build:
      context: ../gaffer-rest/
      args:
        GAFFER_VERSION: ${GAFFER_VERSION}
        ACCUMULO_VERSION: ${ACCUMULO_VERSION}
    ports:
    - 8080:8080
    volumes:
    - ../gaffer/conf/store.properties:/gaffer/store/store.properties:ro
    - ../gaffer-road-traffic-loader/config/graph/graphConfig.json:/gaffer/graph/graphConfig.json:ro
    - ../gaffer-road-traffic-loader/config/schema:/gaffer/schema:ro

  data-loader:
    depends_on:
    - accumulo-master
    image: gchq/gaffer-road-traffic-loader:${GAFFER_VERSION}
    build:
      context: ../gaffer-road-traffic-loader
      args:
        GAFFER_VERSION: ${GAFFER_VERSION}
    restart: on-failure
    volumes:
    - ../gaffer/conf/store.properties:/gaffer/store/store.properties:ro
    - ../gaffer-road-traffic-loader/config/graph/graphConfig.json:/gaffer/graph/graphConfig.json:ro
    - ../gaffer-road-traffic-loader/config/schema:/gaffer/schema:ro

  notebook:
    depends_on:
    - data-loader
    image: gchq/gaffer-pyspark-notebook:${GAFFER_VERSION}
    container_name: notebook
    hostname: notebook
    user: root
    environment:
    - GRANT_SUDO=yes
    - HADOOP_USER_NAME=hadoop
    - HADOOP_CONF_DIR=${HADOOP_CONF_DIR}
    - GAFFER_REST_API_URL=http://gaffer-rest:8080/rest/
    - JUPYTER_ENABLE_LAB=true
    build:
      context: .
      args:
        HADOOP_VERSION: ${HADOOP_VERSION}
        SPARK_VERSION: ${SPARK_VERSION}
        GAFFERPY_VERSION: gaffer-tools-${GAFFERPY_VERSION}
        KUBECTL_VERSION: ${KUBECTL_VERSION}
    ports:
    - 8888:8888
    volumes:
    - ./conf/hadoop:${HADOOP_CONF_DIR}:ro
    - ./conf/spark:/opt/spark/conf:ro
    - ../gaffer/conf/store.properties:/etc/gaffer/store/store.properties:ro
    - ../gaffer-road-traffic-loader/config/graph:/etc/gaffer/graph:ro
    - ../gaffer-road-traffic-loader/config/schema:/etc/gaffer/schema:ro
    - ./examples:/home/jovyan/examples:ro
